import pandas as pd
import os
import glob

output_dir = "./mnt/data/output"


def merge_csv_files(pattern, output_file, id_column=None):
    """
    ÅÄ…czy pliki z output_dir pasujÄ…ce do pattern i zapisuje do output_file
    (Å›cieÅ¼ka wzglÄ™dna wzglÄ™dem output_dir).
    JeÅ›li id_column = None â†’ nie sprawdza duplikatÃ³w ID.
    """
    files = glob.glob(os.path.join(output_dir, pattern))
    if not files:
        print(f"âš  Brak plikÃ³w pasujÄ…cych do wzorca: {pattern}")
        return

    dfs = []
    for file in files:
        try:
            df = pd.read_csv(file)
            dfs.append(df)
        except Exception as e:
            print(f"âŒ BÅ‚Ä…d przy wczytywaniu {file}: {e}")

    if not dfs:
        print(f"âš  Nie wczytano Å¼adnych danych dla {pattern}")
        return

    merged_df = pd.concat(dfs, ignore_index=True)

    # peÅ‚na Å›cieÅ¼ka do pliku wyjÅ›ciowego
    out_path = os.path.join(output_dir, output_file)
    os.makedirs(os.path.dirname(out_path), exist_ok=True)

    merged_df.to_csv(out_path, index=False)
    print(f"âœ… Zapisano poÅ‚Ä…czony plik: {out_path} ({len(merged_df)} rekordÃ³w)")

    # Sprawdzanie unikalnoÅ›ci ID â€“ tylko jeÅ›li podano id_column
    if id_column is None:
        return

    if id_column in merged_df.columns:
        duplicates = merged_df[merged_df.duplicated(subset=[id_column], keep=False)]
        if not duplicates.empty:
            print(f"âŒ UWAGA: {len(duplicates)} powtÃ³rzonych ID w {output_file} (kolumna {id_column})")
            print(duplicates[[id_column]].drop_duplicates())
        else:
            print(f"âœ… Brak duplikatÃ³w w {output_file} dla ID: {id_column}")
    else:
        print(f"âš  Kolumna {id_column} nie istnieje w {output_file}")


def merge_wynik_regat(output_file="all_wynikRegat.csv"):
    """
    ÅÄ…czy wszystkie pliki wynikÃ³w regat w output_dir.
    Wspiera dwa ukÅ‚ady:
      1) pliki obok innych exportÃ³w: *_wynikRegat.csv
      2) osobny folder: output/wynikiRegat/*.csv (nazwy plikÃ³w dowolne)

    ğŸ” NIE sprawdzamy juÅ¼ duplikatÃ³w po kolumnie ID (bo w DB jest AUTO_INCREMENT).
    Opcjonalnie sprawdzamy tylko duplikaty po (regaty, klub).
    """
    # Zbierz kandydatÃ³w rekursywnie
    files = set(glob.glob(os.path.join(output_dir, "**", "*_wynikRegat.csv"), recursive=True))

    # Dodatkowy katalog, jeÅ›li istnieje
    wyniki_dir = os.path.join(output_dir, "wynikiRegat")
    if os.path.isdir(wyniki_dir):
        files.update(glob.glob(os.path.join(wyniki_dir, "*.csv")))

    files = sorted(files)
    if not files:
        print("âš  Brak plikÃ³w wynikÃ³w regat (szukaÅ‚em *_wynikRegat.csv oraz ./wynikiRegat/*.csv)")
        return

    dfs = []
    for f in files:
        try:
            df = pd.read_csv(f)
            # UporzÄ…dkuj nazwy kolumn, jeÅ›li rÃ³Å¼niÄ… siÄ™ wielkoÅ›ciÄ… liter
            cols_lower = {c.lower(): c for c in df.columns}
            rename_map = {}
            if 'id' in cols_lower:                 rename_map[cols_lower['id']] = 'ID'
            if 'regaty' in cols_lower:             rename_map[cols_lower['regaty']] = 'regaty'
            if 'klub' in cols_lower:               rename_map[cols_lower['klub']] = 'klub'
            if 'miejscowregatach' in cols_lower:   rename_map[cols_lower['miejscowregatach']] = 'miejsceWRegatach'
            if rename_map:
                df = df.rename(columns=rename_map)

            # JeÅ›li sÄ… wszystkie cztery kolumny â€“ wybierz je, jeÅ›li nie, zostaw jak jest
            if set(['ID', 'regaty', 'klub', 'miejsceWRegatach']).issubset(df.columns):
                df = df[['ID', 'regaty', 'klub', 'miejsceWRegatach']]
            dfs.append(df)
        except Exception as e:
            print(f"âŒ BÅ‚Ä…d przy wczytywaniu {f}: {e}")

    if not dfs:
        print("âš  Nie wczytano Å¼adnych danych dla wynikÃ³w regat")
        return

    merged_df = pd.concat(dfs, ignore_index=True)

    # Normalizacja typÃ³w
    if 'miejsceWRegatach' in merged_df.columns:
        merged_df['miejsceWRegatach'] = pd.to_numeric(
            merged_df['miejsceWRegatach'], errors='ignore'
        )
    if 'regaty' in merged_df.columns:
        merged_df['regaty'] = pd.to_numeric(merged_df['regaty'], errors='ignore')

    # Zapis
    out_path = os.path.join(output_dir, output_file)
    os.makedirs(os.path.dirname(out_path), exist_ok=True)
    merged_df.to_csv(out_path, index=False)
    print(f"âœ… Zapisano poÅ‚Ä…czony plik: {out_path} ({len(merged_df)} rekordÃ³w)")

    # ğŸ” Duplikaty â€“ juÅ¼ NIE po ID, tylko opcjonalnie po (regaty, klub)
    if set(['regaty', 'klub']).issubset(merged_df.columns):
        duplicates = merged_df[merged_df.duplicated(subset=['regaty', 'klub'], keep=False)]
        if not duplicates.empty:
            print(f"âš  Info: {len(duplicates)} powtÃ³rzeÅ„ kombinacji (regaty, klub) w {output_file}")
            print(duplicates[['regaty', 'klub']].drop_duplicates())
        else:
            print("âœ… Brak duplikatÃ³w po (regaty, klub) w all_wynikRegat.csv")
    else:
        print("â„¹ Pomijam sprawdzanie duplikatÃ³w â€“ brak kolumn (regaty, klub)")


if __name__ == "__main__":
    # wyscigi â€“ sprawdzamy duplikaty po ID_wyscigu
    merge_csv_files(pattern="*_wyscigi.csv",
                    output_file="merge/all_wyscigi.csv",
                    id_column="ID_wyscigu")

    # regaty â€“ sprawdzamy duplikaty po ID_Regat
    merge_csv_files(pattern="*_regaty.csv",
                    output_file="merge/all_regaty.csv",
                    id_column="ID_Regat")

    # miejsca â€“ NIE sprawdzamy duplikatÃ³w ID (id_column=None)
    merge_csv_files(pattern="*_miejsca.csv",
                    output_file="merge/all_miejsca.csv",
                    id_column=None)

    # wyniki regat â€“ brak sprawdzania duplikatÃ³w po ID
    merge_wynik_regat("merge/all_wynikRegat.csv")
